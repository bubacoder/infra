# Open WebUI is an extensible, feature-rich, and user-friendly self-hosted WebUI for various LLM runners, supported LLM runners include Ollama and OpenAI-compatible APIs.  
#
# https://openwebui.com/  
# https://github.com/open-webui/open-webui/  
# https://docs.openwebui.com/getting-started/  
#
# Compose file based on: https://github.com/open-webui/open-webui/blob/main/docker-compose.yaml
---
name: open-webui
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:0.5.7
    container_name: open-webui
    restart: unless-stopped
    volumes:
      - ${DOCKER_VOLUMES}/open-webui:/app/backend/data
    networks:
      - proxy
    # Environment Variable Configuration
    # https://docs.openwebui.com/getting-started/advanced-topics/env-configuration/
    environment:
      WEBUI_SECRET_KEY: ${OPEN_WEBUI_SECRET_KEY}
      WEBUI_URL: "https://open-webui.${MYDOMAIN}"
      OLLAMA_API_BASE_URL: "https://ollama.${MYDOMAIN}/api"
      # Web search using SearXNG
      # Details: https://github.com/open-webui/docs/blob/main/docs/tutorials/integrations/web_search.md
      ENABLE_RAG_WEB_SEARCH: true
      RAG_WEB_SEARCH_ENGINE: "searxng"
      RAG_WEB_SEARCH_RESULT_COUNT: 5
      RAG_WEB_SEARCH_CONCURRENT_REQUESTS: 10
      SEARXNG_QUERY_URL: "https://searxng.${MYDOMAIN}/search?q=<query>&format=json"
    extra_hosts:
      - host.docker.internal:host-gateway
    labels:
      traefik.enable: true
      traefik.http.routers.open-webui.middlewares: localaccess@file
      traefik.http.services.open-webui.loadbalancer.server.port: 8080
      homepage.group: AI
      homepage.name: "Open WebUI"
      homepage.icon: open-webui.png
      homepage.href: https://open-webui.${MYDOMAIN}/
      homepage.description: "User-friendly WebUI for LLMs"

networks:
  proxy:
    external: true
